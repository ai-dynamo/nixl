# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# =============================================================================
# Multi-Stage Docker Build for NIXL
# =============================================================================
#
# This Dockerfile uses a multi-stage build approach:
#
# STAGE 1 (base): Build environment with all dependencies
#   - Used by CI pipeline with --target wheel-base
#   - Contains all system packages, libraries, and build tools
#   - Does NOT build NIXL or generate wheels
#
# STAGE 2 (default): Complete environment with NIXL build and wheels
#   - Used by users building the full image
#   - Inherits from base stage
#   - Builds NIXL from source and generates wheels for all Python versions
#
# Usage:
#   CI Pipeline: docker build --target wheel-base -f contrib/Dockerfile.manylinux .
#   User Build:  docker build -f contrib/Dockerfile.manylinux .
#
# =============================================================================

# Stage 1: Base environment with all dependencies (used by CI)
# This stage provides a clean build environment with all required dependencies
# but does not build NIXL or generate wheels
ARG BASE_IMAGE
ARG BASE_IMAGE_TAG
FROM ${BASE_IMAGE}:${BASE_IMAGE_TAG} as wheel-base

# Build arguments for the base stage
ARG ARCH="x86_64"
ARG UCX_REF="v1.20.x"
ARG LIBFABRIC_VERSION="v1.21.0"
ARG BUILD_TYPE="release"
ARG NPROC=8

# Install system packages and development tools
# These packages provide the foundation for building NIXL and its dependencies
RUN yum groupinstall -y 'Development Tools' &&  \
    dnf install -y almalinux-release-synergy && \
    dnf config-manager --set-enabled powertools && \
    dnf install -y \
    boost \
    boost-devel \
    clang-devel \
    cmake \
    distribution-gpg-keys-copr \
    dkms \
    flex \
    gflags \
    glibc-headers \
    gcc-c++ \
    libaio \
    libaio-devel \
    libtool-ltdl \
    ninja-build \
    openssl \
    openssl-devel \
    protobuf-compiler \
    protobuf-c-devel \
    protobuf-devel \
    libibverbs \
    libibverbs-devel \
    rdma-core \
    rdma-core-devel \
    libibumad \
    libibumad-devel \
    numactl-devel \
    librdmacm-devel \
    hwloc \
    hwloc-devel \
    wget \
    zlib

# =============================================================================
# Build and install OpenSSL 3.x from source
# =============================================================================
# OpenSSL 3.x is required for modern security features and compatibility
# We build it from source to ensure consistent behavior across environments
RUN yum install -y perl-IPC-Cmd perl-Test-Simple perl-Data-Dumper
RUN cd /tmp && \
    wget -q https://www.openssl.org/source/openssl-3.0.16.tar.gz && \
    tar -xzf openssl-3.0.16.tar.gz && \
    cd openssl-3.0.16 && \
    ./Configure --prefix=/usr/local/openssl3 --openssldir=/usr/local/openssl3 \
        shared zlib linux-$ARCH && \
    make -j${NPROC:-$(nproc)} && \
    make install_sw && \
    echo "/usr/local/openssl3/lib64" > /etc/ld.so.conf.d/openssl3.conf && \
    echo "/usr/local/openssl3/lib" >> /etc/ld.so.conf.d/openssl3.conf && \
    ldconfig && \
    rm -rf /tmp/openssl-3.0.16*

# =============================================================================
# Configure environment to use the custom OpenSSL 3.x installation
# =============================================================================
# These environment variables ensure that all subsequent builds use our custom
# OpenSSL installation instead of the system version
ENV PKG_CONFIG_PATH="/usr/local/openssl3/lib64/pkgconfig:/usr/local/openssl3/lib/pkgconfig:$PKG_CONFIG_PATH"
ENV LD_LIBRARY_PATH="/usr/local/openssl3/lib64:/usr/local/openssl3/lib:$LD_LIBRARY_PATH"
ENV OPENSSL_ROOT_DIR="/usr/local/openssl3"
ENV OPENSSL_LIBRARIES="/usr/local/openssl3/lib64:/usr/local/openssl3/lib"
ENV OPENSSL_INCLUDE_DIR="/usr/local/openssl3/include"

# =============================================================================
# Build and install gRPC and related networking libraries
# =============================================================================
# gRPC is required for high-performance networking in NIXL
WORKDIR /workspace

# Build gRPC v1.73.0 with SSL support and shared libraries
RUN git clone --recurse-submodules -b v1.73.0 --depth 1 --shallow-submodules https://github.com/grpc/grpc && \
    cd grpc && \
    mkdir -p cmake/build && \
    cd cmake/build && \
    cmake -DgRPC_INSTALL=ON \
    -DgRPC_BUILD_TESTS=OFF \
    -DBUILD_SHARED_LIBS=ON \
    -DCMAKE_CXX_STANDARD=17 \
    -DCMAKE_BUILD_TYPE=Release \
    -DCMAKE_INSTALL_PREFIX=/usr/local \
    -DgRPC_SSL_PROVIDER=package ../.. && \
    make -j${NPROC:-$(nproc)} && \
    make install

ENV LD_LIBRARY_PATH=/usr/local/lib:/usr/local/lib64:$LD_LIBRARY_PATH

# Build etcd-cpp-apiv3 for distributed coordination
RUN cd /workspace && \
    git clone --depth 1 https://github.com/etcd-cpp-apiv3/etcd-cpp-apiv3.git && \
    cd etcd-cpp-apiv3 && \
    sed -i '/^find_dependency(cpprestsdk)$/d' etcd-cpp-api-config.in.cmake && \
    mkdir build && cd build && \
    cmake .. -DBUILD_ETCD_CORE_ONLY=ON -DCMAKE_BUILD_TYPE=Release && make -j${NPROC:-$(nproc)} && make install

# The base image libcurl is linked against openssl 1.x, so we need to build from source
# in order to use openssl 3.x. This is needed to build aws-sdk-cpp.
RUN wget https://curl.se/download/curl-8.5.0.tar.gz && \
    tar xzf curl-8.5.0.tar.gz && cd curl-8.5.0 && \
    ./configure --prefix=/usr/local --with-ssl=/usr/local/openssl3 --enable-shared && \
    make -j${NPROC:-$(nproc)} && make install

RUN git clone --recurse-submodules --depth 1 --shallow-submodules https://github.com/aws/aws-sdk-cpp.git --branch 1.11.581
RUN mkdir aws_sdk_build && cd aws_sdk_build && \
    export LDFLAGS="-L/usr/local/openssl3/lib64 -L/usr/local/openssl3/lib" && \
    export CFLAGS="-I/usr/local/openssl3/include" && \
    export CXXFLAGS="-I/usr/local/openssl3/include" && \
    cmake ../aws-sdk-cpp/ -DCMAKE_BUILD_TYPE=Release -DBUILD_ONLY="s3" -DENABLE_TESTING=OFF -DCMAKE_INSTALL_PREFIX=/usr/local \
        -DCMAKE_PREFIX_PATH="/usr/local/openssl3;/usr/local" \
        -DCURL_LIBRARY=/usr/local/lib/libcurl.so \
        -DCURL_INCLUDE_DIR=/usr/local/include \
        -DOPENSSL_USE_STATIC_LIBS=OFF && \
    make -j${NPROC:-$(nproc)} && make install

RUN git clone https://github.com/nvidia/gusli.git && \
     cd gusli && \
     sed -i '/^LFLAGS_ALL =/{/-lrt/ b; s/$/ -lrt/}' Makefile && \
     make all BUILD_RELEASE=1 BUILD_FOR_UNITEST=0 VERBOSE=1 ALLOW_USE_URING=0 && \
     cd ..

# =============================================================================
# Install UV package manager and Rust toolchain
# =============================================================================
# UV is used for Python package management and building wheels
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

# Set up Rust environment for native dependencies
ENV RUSTUP_HOME=/usr/local/rustup \
    CARGO_HOME=/usr/local/cargo \
    PATH=/usr/local/cargo/bin:$PATH \
    RUST_VERSION=1.86.0 \
    RUSTARCH=${ARCH}-unknown-linux-gnu

RUN wget --tries=3 --waitretry=5 "https://static.rust-lang.org/rustup/archive/1.28.1/${RUSTARCH}/rustup-init" && \
    chmod +x rustup-init && \
    ./rustup-init -y --no-modify-path --profile minimal --default-toolchain $RUST_VERSION --default-host ${RUSTARCH} && \
    case "$ARCH" in \
        aarch64) RUSTUP_SHA256="c64b33db2c6b9385817ec0e49a84bcfe018ed6e328fe755c3c809580cc70ce7a" ;; \
        x86_64) RUSTUP_SHA256="a3339fb004c3d0bb9862ba0bce001861fe5cbde9c10d16591eb3f39ee6cd3e7f" ;; \
        *) echo "Unsupported architecture for Rust: $ARCH" && exit 1 ;; \
    esac && \
    echo "$RUSTUP_SHA256 *rustup-init" | sha256sum -c - && \
    rm rustup-init && \
    chmod -R a+w $RUSTUP_HOME $CARGO_HOME

RUN wget --tries=3 --waitretry=5 https://www.mellanox.com/downloads/DOCA/DOCA_v3.2.0/host/doca-host-3.2.0-125000_25.10_rhel8.${ARCH}.rpm -O doca-host.rpm && \
    rpm -i doca-host.rpm && \
    dnf install -y libnl3-devel && \
    cd /usr/share/doca-host-3.2.0/repo/Packages/ && \
    rpm -ivh --nodeps doca-sdk-common-*rpm && \
    rpm -ivh --nodeps doca-sdk-rdma-*rpm && \
    rpm -ivh --nodeps doca-sdk-verbs-*rpm && \
    rpm -ivh --nodeps doca-sdk-gpunetio-*rpm && \
    # Check that gpunetio development package is installed correctly
    pkg-config --cflags --libs doca-gpunetio

# Update library path for Rust-built libraries
ENV LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH

# Set CUDA path for GPU support
ENV CUDA_PATH=/usr/local/cuda

WORKDIR /workspace
# =============================================================================
# Build and install UCX (Unified Communication X) and gdrcopy
# =============================================================================
# UCX provides high-performance networking primitives
# gdrcopy enables GPU memory access from CPU

# Remove any existing UCX installations to avoid conflicts
RUN rm -rf /usr/lib/ucx
RUN rm -rf /opt/hpcx/ucx

# Build and install gdrcopy for GPU memory access
RUN cd /workspace && \
    git clone --depth 1 https://github.com/NVIDIA/gdrcopy.git && \
    cd gdrcopy && \
    git fetch --tags --depth=1 && \
    latest_tag=$(git describe --tags "$(git rev-list --tags --max-count=1)") && \
    git checkout "$latest_tag" && \
    cd packages && \
    CUDA=/usr/local/cuda ./build-rpm-packages.sh && \
    rpm -Uvh gdrcopy-kmod-*.el8.noarch.rpm && \
    rpm -Uvh gdrcopy-*.el8.$ARCH.rpm && \
    rpm -Uvh gdrcopy-devel-*.el8.noarch.rpm

# Build and install UCX with CUDA, verbs, and gdrcopy support
# This provides the networking primitives needed for high-performance communication
RUN cd /usr/local/src && \
     git clone https://github.com/openucx/ucx.git && \
     cd ucx && 			     \
     git checkout $UCX_REF &&	     \
     ./autogen.sh && \
     ./contrib/configure-release-mt  \
         --enable-shared             \
         --disable-static            \
         --disable-doxygen-doc       \
         --enable-optimizations      \
         --enable-cma                \
         --enable-devel-headers      \
         --with-cuda=/usr/local/cuda \
         --with-verbs                \
         --with-dm                   \
         --with-gdrcopy=/usr/local   \
         --with-efa               && \
     make -j${NPROC:-$(nproc)} &&                      \
     make -j${NPROC:-$(nproc)} install-strip &&        \
     ldconfig

# Build libfabric from source
RUN wget --tries=3 --waitretry=5 --timeout=30 --read-timeout=60 \
    "https://github.com/ofiwg/libfabric/releases/download/${LIBFABRIC_VERSION}/libfabric-${LIBFABRIC_VERSION#v}.tar.bz2" -O libfabric.tar.bz2 && \
    tar xjf libfabric.tar.bz2 && rm libfabric.tar.bz2 && \
    cd libfabric-* && \
    ./autogen.sh && \
    ./configure --prefix=/usr/local \
                --disable-verbs \
                --disable-psm3 \
                --disable-opx \
                --disable-usnic \
                --disable-rstream \
                --enable-efa \
                --with-cuda=/usr/local/cuda \
                --enable-cuda-dlopen \
                --with-gdrcopy \
                --enable-gdrcopy-dlopen && \
    make -j${NPROC:-$(nproc)} && \
    make install && \
    ldconfig

# =============================================================================
# Stage 2: Default stage with NIXL build and wheel generation
# =============================================================================
# This stage is used by users who want a complete environment with NIXL built
# and wheels generated. It inherits all dependencies from the base stage.
#
# Usage: docker build -f contrib/Dockerfile.manylinux .
# (No --target flag needed, this is the default stage)
FROM wheel-base

# Build arguments for the default stage
ARG DEFAULT_PYTHON_VERSION="3.12"
ARG ARCH="x86_64"

# Copy NIXL source code into the container
# By default, uv downloads python packages to $HOME/.cache/uv and hard links them
# from the virtual environment. This means that the files reside in /root/.cache/uv,
# which is not what we want since some systems mount user home dir into /root,
# in which case the venv is broken when the container is started.
# Set a custom cache directory inside /workspace to avoid this.
ENV UV_CACHE_DIR=/workspace/.cache/uv
RUN mkdir -p $UV_CACHE_DIR
# Create a new virtual environment
ENV VIRTUAL_ENV=/workspace/.venv
RUN rm -rf $VIRTUAL_ENV && uv venv $VIRTUAL_ENV --python $DEFAULT_PYTHON_VERSION
# Activate the virtual environment
ENV PATH="$VIRTUAL_ENV/bin:$PATH"
# Install python dependencies
RUN uv pip install --upgrade meson meson-python pybind11 patchelf pyYAML click setuptools tabulate auditwheel tomlkit
# Install PyTorch
RUN export UV_INDEX="https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d .)" && \
    uv pip install torch torchvision torchaudio

COPY . /workspace/nixl
WORKDIR /workspace/nixl

# =============================================================================
# Build NIXL from source using meson build system
# =============================================================================
# This step compiles NIXL with CUDA support and installs it to /usr/local/nixl
RUN rm -rf build && \
    mkdir build && \
    meson setup build/ --prefix=/usr/local/nixl --buildtype=$BUILD_TYPE \
    -Dcudapath_lib="/usr/local/cuda/lib64" \
    -Dcudapath_inc="/usr/local/cuda/include" && \
    cd build && \
    ninja && \
    ninja install

# =============================================================================
# Configure library paths and system configuration for NIXL
# =============================================================================
# Set up environment variables for NIXL libraries and plugins
ENV LD_LIBRARY_PATH=/usr/local/nixl/lib64/:$LD_LIBRARY_PATH
ENV LD_LIBRARY_PATH=/usr/local/nixl/lib64/plugins:$LD_LIBRARY_PATH
ENV NIXL_PLUGIN_DIR=/usr/local/nixl/lib64/plugins

# Configure system to find NIXL libraries at runtime
RUN echo "/usr/local/nixl/lib/$ARCH-linux-gnu" > /etc/ld.so.conf.d/nixl.conf && \
    echo "/usr/local/nixl/lib/$ARCH-linux-gnu/plugins" >> /etc/ld.so.conf.d/nixl.conf && \
    ldconfig

# =============================================================================
# Generate Python wheels for distribution
# =============================================================================
# This section creates Python wheels for all supported Python versions
# The wheels include all necessary native libraries and plugins
#
# Note: No need to specifically add path to libcuda.so here, meson finds
# the stubs and links them automatically during the build process
ARG WHL_PYTHON_VERSIONS="3.9,3.10,3.11,3.12,3.13"
ARG WHL_PLATFORM="manylinux_2_28_$ARCH"
# Build wheels for each Python version
# This creates distributable wheels that can be installed on compatible systems
RUN IFS=',' read -ra PYTHON_VERSIONS <<< "$WHL_PYTHON_VERSIONS" && \
    export UV_INDEX="https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d .)" && \
    mkdir -p dist && \
    for PYTHON_VERSION in "${PYTHON_VERSIONS[@]}"; do \
        export PATH=$VIRTUAL_ENV/bin:$PATH && \
        ./contrib/build-wheel.sh \
            --python-version $PYTHON_VERSION \
            --platform $WHL_PLATFORM \
            --ucx-plugins-dir /usr/lib64/ucx \
            --nixl-plugins-dir $NIXL_PLUGIN_DIR \
            --output-dir dist ; \
    done

# Copy the meta package wheel to the dist directory, which will be used to push to PyPI.
# Only do this for the CUDA 12 builds, since by default nixl depends on nixl-cu12.
RUN if [ "$(echo $CUDA_VERSION | cut -d. -f1)" = "12" ]; then \
      cp build/src/bindings/python/nixl-meta/nixl*.whl dist/; \
    fi

RUN uv pip install dist/nixl*cp${DEFAULT_PYTHON_VERSION//./}*.whl build/src/bindings/python/nixl-meta/nixl*.whl
